{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "irNpUtJgI7wy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    x1=x[0]\n",
        "    x2=x[1]\n",
        "    r1=100*(x2-(x1**2))**2\n",
        "    r2=(1-x1)**2\n",
        "    result=r1+r2\n",
        "    return result\n",
        "\n",
        "def gradf(x):\n",
        "    x1=x[0]\n",
        "    x2=x[1]\n",
        "    g1=(-400*x1*(x2-(x1**2)))-2*(1-x1)\n",
        "    g2=200*(x2-(x1**2))\n",
        "    g=np.array([g1,g2])\n",
        "    return g\n",
        "    \n",
        "def hessian(x):\n",
        "    x1=x[0]\n",
        "    x2=x[1]\n",
        "    result_1=[-400*x2+1200*(x1**2)+2, -400*x1]\n",
        "    result_2=[-400*x1,200]\n",
        "    result=np.array([result_1,result_2])\n",
        "    return result"
      ],
      "metadata": {
        "id": "OIel3gJ4JVUE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HCwWFqYILK-",
        "outputId": "9ad6870f-f3c6-4599-c367-15000cd7eeda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration:  1 \n",
            "x:  [ 0.50000513 -0.49883248]  f(x) 56.3257712008902 \n",
            "||gradf||: 211.09842835012444\n",
            "prev_x:  [ 0.5 -0.5]  f(prev_x):  56.5\n",
            "B:  [[0.00331126 0.00331126]\n",
            " [0.00331126 0.00831126]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  2 \n",
            "x:  [ 0.50001028 -0.49766494]  f(x) 56.15181261561272 \n",
            "||gradf||: 210.77074667082246\n",
            "prev_x:  [ 0.50000513 -0.49883248]  f(prev_x):  56.3257712008902\n",
            "B:  [[0.00331636 0.0033164 ]\n",
            " [0.0033164  0.00831643]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  3 \n",
            "x:  [ 0.50001544 -0.49649738]  f(x) 55.978124244094715 \n",
            "||gradf||: 210.44306498635294\n",
            "prev_x:  [ 0.50001028 -0.49766494]  f(prev_x):  56.15181261561272\n",
            "B:  [[0.00332149 0.00332155]\n",
            " [0.00332155 0.00832162]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  4 \n",
            "x:  [ 0.50002062 -0.49532981]  f(x) 55.80470608626287 \n",
            "||gradf||: 210.11538329669182\n",
            "prev_x:  [ 0.50001544 -0.49649738]  f(prev_x):  55.978124244094715\n",
            "B:  [[0.00332662 0.00332673]\n",
            " [0.00332673 0.00832683]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  5 \n",
            "x:  [ 0.50002582 -0.49416222]  f(x) 55.63155814204352 \n",
            "||gradf||: 209.7877016018154\n",
            "prev_x:  [ 0.50002062 -0.49532981]  f(prev_x):  55.80470608626287\n",
            "B:  [[0.00333178 0.00333191]\n",
            " [0.00333191 0.00833205]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  6 \n",
            "x:  [ 0.50003103 -0.49299462]  f(x) 55.45868041136236 \n",
            "||gradf||: 209.4600199016992\n",
            "prev_x:  [ 0.50002582 -0.49416222]  f(prev_x):  55.63155814204352\n",
            "B:  [[0.00333695 0.00333712]\n",
            " [0.00333712 0.00833729]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  7 \n",
            "x:  [ 0.50003626 -0.491827  ]  f(x) 55.286072894144816 \n",
            "||gradf||: 209.13233819631915\n",
            "prev_x:  [ 0.50003103 -0.49299462]  f(prev_x):  55.45868041136236\n",
            "B:  [[0.00334213 0.00334234]\n",
            " [0.00334234 0.00834255]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  8 \n",
            "x:  [ 0.5000415  -0.49065936]  f(x) 55.113735590315756 \n",
            "||gradf||: 208.80465648565075\n",
            "prev_x:  [ 0.50003626 -0.491827  ]  f(prev_x):  55.286072894144816\n",
            "B:  [[0.00334733 0.00334758]\n",
            " [0.00334758 0.00834782]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  9 \n",
            "x:  [ 0.50004676 -0.48949171]  f(x) 54.94166849979957 \n",
            "||gradf||: 208.47697476966937\n",
            "prev_x:  [ 0.5000415  -0.49065936]  f(prev_x):  55.113735590315756\n",
            "B:  [[0.00335255 0.00335283]\n",
            " [0.00335283 0.00835311]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  10 \n",
            "x:  [ 0.50005204 -0.48832404]  f(x) 54.769871622520185 \n",
            "||gradf||: 208.14929304835024\n",
            "prev_x:  [ 0.50004676 -0.48949171]  f(prev_x):  54.94166849979957\n",
            "B:  [[0.00335778 0.0033581 ]\n",
            " [0.0033581  0.00835841]]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Converged! The minimial solution occurs @x =  [0.96096287 0.92302572] \n",
            "with value f(x)= 0.0015418681663951273 \n",
            "and ||gradf||= 0.11996438409353444\n"
          ]
        }
      ],
      "source": [
        "def inverse_of_regularised_hessian(A,epsilon = 0.01):\n",
        "    try:\n",
        "        Ainv = np.linalg.inv(A)\n",
        "    except:\n",
        "        # find inverse of A + epsilon * I\n",
        "        I = np.ones(A.shape)\n",
        "        Ainv = np.linalg.inv(A + epsilon * I)\n",
        "    return Ainv\n",
        "    \n",
        "\n",
        "def find_best_step_size(f,gradf,x,d,alpha,beta,sigma):\n",
        "    found_alpha = False\n",
        "    while not found_alpha:\n",
        "        if f(x + alpha * d) <= f(x) + sigma * alpha * np.dot(gradf(x),d):\n",
        "            found_alpha = True\n",
        "        else:\n",
        "            alpha = beta * alpha\n",
        "    return alpha\n",
        "\n",
        "def gradient_descent_newton(f,x0,gradf,hessian,\n",
        "                            convergence_thresh=0.12,\n",
        "                            beta=0.8,\n",
        "                            sigma=0.6,\n",
        "                            max_iters=10**5,\n",
        "                            print_progress=True):\n",
        "    \"\"\"\n",
        "    @f is the function to be minimized\n",
        "    @gradf is the gradient of the function to be minimized\n",
        "    @x0 is the initial guess\n",
        "    @convergence_thresh is the threshold used to determine convergence\n",
        "    @max_iters is the maximum number of iteration we try before giving up\n",
        "    @print_progress flag to indicate whether to print the progress of the algorithm \n",
        "    @beta, @sigma are parameters to the optimizer algorithm that finds best step size.\n",
        "    \"\"\"\n",
        "    converged = False\n",
        "    num_iters_so_far = 0\n",
        "    x = x0\n",
        "    trajectory = []\n",
        "    alpha = 1 # initial guess of step size\n",
        "    while not converged:\n",
        "        # find descent direction\n",
        "        d = - gradf(x) / np.linalg.norm(gradf(x))\n",
        "        \n",
        "        # choose step size\n",
        "        alpha = find_best_step_size(f,gradf,x,d,alpha,beta,sigma)\n",
        "        \n",
        "        \n",
        "        # compute regularized hessian\n",
        "        B = inverse_of_regularised_hessian(hessian(x))\n",
        "        \n",
        "        # update x\n",
        "        num_iters_so_far += 1\n",
        "        trajectory.append(x)\n",
        "        x = x + alpha * B@d\n",
        "        \n",
        "        # check convergence\n",
        "        grad_f_norm = np.linalg.norm(gradf(x))\n",
        "        if grad_f_norm <= convergence_thresh:\n",
        "            converged = True \n",
        "            trajectory.append(x)\n",
        "            print(\"\\nConverged! The minimial solution occurs @x = \", x, \"\\nwith value f(x)=\",f(x), \"\\nand ||gradf||=\",grad_f_norm)\n",
        "        if not converged and num_iters_so_far > max_iters:\n",
        "            converged = True\n",
        "            print(\"Failed to converge :(\")\n",
        "            \n",
        "        # output progress\n",
        "        if print_progress and num_iters_so_far<=10:\n",
        "            print(\"\\nIteration: \", num_iters_so_far,\"\\nx: \",x, \" f(x)\", f(x), \"\\n||gradf||:\",grad_f_norm) \n",
        "            print(\"prev_x: \", trajectory[-1], \" f(prev_x): \",f(trajectory[-1]))\n",
        "            print(\"B: \",B)\n",
        "            print(\"--------------------------------------------------------\")\n",
        "    return x,np.array(trajectory) \n",
        "\n",
        "x = np.array([0.5,-0.5])\n",
        "x_opt,trajectory = gradient_descent_newton(f,x,gradf,hessian)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtwF4ct_ZsAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}